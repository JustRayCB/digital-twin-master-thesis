# Current Work Focus (Preprocessing Module)

The project's current focus is the **Data Preprocessing & Quality**
phase (corresponding to Phase P1 in the roadmap, Oct-Dec 2025). All
development effort is concentrated on the **preprocessing-module**
branch, which is building out the real-time data cleaning pipeline and
related infrastructure. This involves improving data quality from the
moment it's collected to the point where it's stored and used for
analysis. Key work streams and recent changes in this area include:

- **Event Schema & Contracts:** Defining a rigorous **schema for sensor data events**. Each reading published to Kafka now adheres to a schema (using Avro or JSON schema) specifying fields like sensor type, units, calibration status, quality flags, timestamps, etc. This ensures all components interpret data consistently. A versioning mechanism is being implemented so that schema changes can be managed without breaking consumers. Alongside this, unit tests were added to validate that incoming messages conform to the schema and that schema evolution is handled correctly.
- **Sensor Data Validation:** Implementing real-time **validation rules** to catch bad or anomalous readings before they propagate. This includes **range checks** for each sensor (e.g., ignoring or flagging humidity readings below 0% or above 100% which are physically impossible), and **physical rate-of-change guards**. The system checks how much a sensor value can realistically change in a short time; a sudden jump in temperature by 30°C in one second, for instance, would be marked as an error or outlier. It also detects **stuck sensors** or flatline data (if a sensor reports the exact same value for an extended period, it might be malfunctioning). These rules are being coded into the preprocessing Spark job. Recent commits likely include threshold constants and logic for each of these checks, as well as test cases simulating sensor faults to ensure the validators work.
- **Missing Data Handling:** Developing strategies to handle gaps when sensors fail to report or readings are dropped. The preprocessing module now includes a **forward-fill with a cap** mechanism and **linear interpolation** for missing data within short windows. For example, if one reading is missed, the system can carry forward the last known good value for a brief period; if a longer outage occurs (beyond a defined max gap), it stops filling and marks the data as missing. Outage periods are marked with special **quality flags** so that downstream analyses know that data was interpolated or absent. This feature was recently added to ensure continuity in time series while being transparent about data quality.
- **Noise Filtering & Drift Correction:** Implementing filters to smooth out sensor noise and slow drift. The team has introduced an **Exponential Weighted Moving Average (EWMA)** filter on certain sensor streams to reduce short-term fluctuations. For example, light sensor readings can spike momentarily; the EWMA provides a smoothed value that is more stable for triggering alerts. There's also an optional **Kalman filter** in consideration for sensors like temperature, which can help separate true signal from noise if needed. Additionally, a running **z-score based anomaly detection** is being trialed: as data comes in, the Spark job computes how many standard deviations a new reading is from a rolling mean, and if it's beyond a threshold (e.g., >3σ), it flags it as a potential outlier. This helps catch gradual sensor drift or sudden aberrant readings that still fall within "valid range" but are statistically abnormal. Recent changes in the code include adding these computations to the streaming job and perhaps a configuration file (preprocessing-spec.md) documenting these transform parameters.
- **Calibration and Normalization:** The preprocessing branch is incorporating sensor **calibration adjustments**. For example, the soil moisture sensor (STEMMA) gives raw values which are now being normalized between 0 and 1 based on calibration points (dry and saturated readings). The team created a calibration table for each sensor type so that the raw measurements can be translated into standardized units or percentages. Code has been added to apply these calibration offsets or scaling factors in real-time. This ensures that, say, "moisture=1.0" corresponds to truly saturated soil across devices. Having normalized data is crucial for any ML models and alert thresholds to be meaningful.
- **Aggregations (Rollups):** To facilitate easier analysis and reduce storage volume, the current work includes computing **rolling aggregates** of sensor data. The preprocessing module (or a companion batch job) now computes 1-minute, 5-minute, and 1-hour aggregates of the readings. For instance, it will produce a 1-minute average of temperature or a 5-minute max of light intensity. These rollups can be continuously computed using Spark and then written to InfluxDB (or downsampled in Influx itself via Continuous Queries). The InfluxDB retention policy and continuous query setup was likely updated (as per deliverables) to automatically keep fine-grained data for a period (e.g., raw data for 7 days) and aggregates for longer. Recent commits might include an influxdb.conf or scripts to create these retention policies. This aggregation work will allow the dashboard to efficiently query longer time spans (like a monthly trend) without overloading on raw data points.
- **Alerting Engine:** A basic **rule-based alerting system** is under development. The preprocessing pipeline (in collaboration with the AI module) now monitors the cleaned data for threshold conditions. For example, if soil moisture drops below a defined threshold for more than X minutes, an alert event is generated. The initial implementation uses simple thresholds and a **persistence window** (the condition must hold for a certain time to be considered an alert, to avoid flapping). The branch includes code for **alert de-duplication and cool-down** - once an alert is triggered (e.g., "Soil too dry"), it won't spam multiple alerts for the same condition; it will wait either until the condition clears and re-occurs or a cooldown period passes. These alerts are published to a Kafka alerts topic and also inserted into the audit log. The web dashboard has a section (perhaps newly added) to display active alerts and their statuses. Recent changes likely added an alerts.py or similar module to define these rules, as well as configuration in a thresholds.md document listing the current threshold values for each sensor (which was mentioned as a deliverable in P0).
- **Action & Audit Logging:** A significant current effort is setting up the **audit log system** to record every important event in a structured way. The team has designed an **Entity-Relationship Diagram (ERD)** for tables like actions, alerts, configs, etc., and is implementing these schemas in an SQLite database (with migration scripts ready to target PostgreSQL). For example, when an alert triggers, a new row is added to the alerts table with details; when in the future an actuator is activated, an actions entry will be logged. Each entry includes metadata like timestamp, actor (could be "system" or a user), and crucially the **correlation_id** to link it to the originating sensor event and any resulting actions. The current branch includes writing a Kafka sink connector or custom code to listen to alerts and actions topics and insert records into this relational store. This ensures persistence of these events even if the system restarts and provides an audit trail for debugging and analysis. The groundwork (SQLAlchemy models and Alembic migrations) is being laid now, though some parts (like actuator commands) will fully come into play in the next phase.
- **APIs and Minimal UI for Logs:** To expose the new data (alerts, actions, config changes) and support the operator's visibility, the team is adding **REST API endpoints** and simple UI pages in the dashboard. Endpoints like /logs, /alerts, /actions, /configs are being developed to retrieve recent entries from the audit log or current system configuration. On the Flask dashboard, new pages or sections show tables of recent alerts and actions, possibly with filtering or export functionality (e.g., CSV export was mentioned). This is a "minimal UI" for now - just enough to verify that the backend is logging events correctly and to allow a user to inspect what the system has been doing. The active branch likely contains new Flask route definitions and HTML templates for these tables. By implementing this now, the team can test end-to-end that when, say, an alert rule triggers, it appears on the dashboard and is stored in the database. This also sets the stage for the more feature-rich UI planned in Phase P4.
- **Quality Assurance & Testing:** As the preprocessing features roll out, a lot of effort goes into testing the system with realistic and extreme conditions. The current focus includes **replaying synthetic sensor data** through the pipeline to verify everything works as expected. The team has created test scenarios (perhaps using recorded data or generated patterns) including normal operation, sensor dropout, wildly noisy data, etc., to see how the system copes. They are calculating a **Data Quality (DQ) score** for each message or each batch of messages, which might combine factors like how many validation rules passed, whether it was interpolated, etc., to quantitatively track data quality improvements. Recent changes likely include a test harness (maybe a Jupyter notebook or a small Python script) that publishes test data to Kafka topics and then checks the outputs in InfluxDB and the alerts log. Bug fixes and refinements are being made based on these tests (for example, tweaking threshold values or filter parameters). The CI pipeline might also run some of these integration tests in a containerized environment (using a local Kafka and InfluxDB instance) to ensure that new commits don't break the streaming job.

In summary, the **preprocessing-module branch** is rapidly evolving the system's ability to ensure **data integrity and readiness** for analysis. The recent commits and work are all about cleaning data, structuring it, and providing transparency (via logs and alerts) into the system's operation. By the end of this phase (targeted Dec 2025), the goal is to have the preprocessing service **production-ready, with validation and alerting live**. This will set a strong foundation for the subsequent phase, where the focus will shift to using this clean data in a **feedback control loop** (automating watering, etc.). But for now, the team is concentrating on getting the data pipeline correct and reliable. The progress so far has been promising: the digital twin can ingest raw sensor readings, filter out bad data, flag anomalies, aggregate trends, and even notify users of potential issues, all in real time. These capabilities being put in place now will make the upcoming addition of control and ML much smoother and more effective.
